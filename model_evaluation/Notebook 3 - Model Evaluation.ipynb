{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "# import decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# import functionality to split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import functionality for creating stratified k-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# import metrics module to construct confusion matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "# function for balancing dataset using oversampling and undersampling\n",
    "def resample(x_train, y_train):\n",
    "    \n",
    "    # use both oversampling (of minority class - fake reviews) and oversampling (of majority class - genuine reviews)\n",
    "    # The resampling will be random (we are not picking out specific reviews)\n",
    "\n",
    "    # 1. \"combine\" x_train and y_train\n",
    "    x_train = x_train.copy()\n",
    "    x_train[\"label\"] = y_train\n",
    "\n",
    "    # create a sub-dataframes\n",
    "    genuine = x_train[x_train[\"label\"] == \"N\"]\n",
    "    fake = x_train[x_train[\"label\"] == \"Y\"]\n",
    "\n",
    "    #print(\"Number of fake and genuine reviews before re-sampling: \")\n",
    "    #print(\"Fake: \", len(fake))\n",
    "    #print(\"Genuine: \", len(genuine))\n",
    "    #print(\"Total: \", len(genuine) + len(fake))\n",
    "\n",
    "    # 1. oversampling\n",
    "\n",
    "    # 2x oversampling\n",
    "    fake_copy = fake.copy()\n",
    "    fake = pd.concat([fake, fake_copy])\n",
    "\n",
    "    #print(\"Number of fake and genuine reviews after over-sampling: \")\n",
    "    #print(\"Fake: \", len(fake))\n",
    "    #print(\"Genuine: \", len(genuine))\n",
    "    #print(\"Total: \", len(genuine) + len(fake))\n",
    "\n",
    "    # 2. undersampling\n",
    "\n",
    "    # shuffle rows\n",
    "    genuine = genuine.reindex(np.random.permutation(genuine.index))\n",
    "    # under-sample majority class\n",
    "    genuine = genuine[:len(fake)]\n",
    "\n",
    "    # combine two sub-dataframes\n",
    "    x_train = pd.concat([genuine, fake])\n",
    "    x_train = x_train.reset_index(drop = True)\n",
    "\n",
    "    #print(\"Number of fake and genuine reviews after re-sampling: \")\n",
    "    #print(\"Fake: \", len(fake))\n",
    "    #print(\"Genuine: \", len(genuine))\n",
    "    #print(\"Total: \", len(genuine) + len(fake))\n",
    "\n",
    "    # shuffle rows\n",
    "    x_train = x_train.reindex(np.random.permutation(x_train.index))\n",
    "\n",
    "    # 3. \"separate\" x_train and y_train again\n",
    "    y_train = pd.Series(x_train[\"label\"])\n",
    "    x_train = x_train.drop(\"label\", 1)\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "# Load dataframe for model evaluation\n",
    "\n",
    "# load dataframe with 11 features ready (hasProfile missing)\n",
    "#path = \"/Users/artembutbaev/OneDrive/University of Bath 20-21 (Year 4)/CM - Individual Project/2. Code/Model Building/df2.pkl\"\n",
    "#df = pd.read_pickle(path)\n",
    "# load dataframe with all 15 features ready\n",
    "path = \"/Users/artembutbaev/OneDrive/University of Bath 20-21 (Year 4)/CM - Individual Project/2. Code/Model Building/df4.pkl\"\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake and genuine review examples included in dissertation\n",
    "# df[df['flagged'] == 'N'].iloc[2333].reviewContent\n",
    "# df[df['flagged'] == 'Y'].iloc[233].reviewContent\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.Series(df[\"flagged\"])\n",
    "\n",
    "# feature set 1 - review-centric features only\n",
    "columns = [df[\"max_tfidf\"], df[\"reviewLength\"], df[\"numCount\"], \n",
    "           df[\"symCount\"], df[\"adjProp\"], df[\"avgSentiment\"], df[\"nounProp\"]]\n",
    "headers = [\"max_tfidf\", \"reviewLength\", \"numCount\", \n",
    "           \"symCount\", \"adjProp\", \"avgSentiment\", \"nounProp\"]\n",
    "fs1 = pd.concat(columns, axis=1, keys=headers)\n",
    "\n",
    "# feature set 2 - reviewer-centric features only\n",
    "columns = [df[\"hasProfile\"], df[\"postCount\"], df[\"usefulCount\"], \n",
    "           df[\"coolCount\"], df[\"funnyCount\"], df[\"maxReviews\"]]\n",
    "headers = [\"hasProfile\", \"postCount\", \"usefulCount\", \n",
    "           \"coolCount\", \"funnyCount\", \"maxReviews\"]\n",
    "fs2 = pd.concat(columns, axis=1, keys=headers)\n",
    "\n",
    "# feature set 3 - all features\n",
    "columns = [df[\"max_tfidf\"], df[\"reviewLength\"], df[\"numCount\"], \n",
    "           df[\"symCount\"], df[\"adjProp\"], df[\"avgSentiment\"], df[\"nounProp\"], \n",
    "           df[\"hasProfile\"], df[\"postCount\"], df[\"usefulCount\"], \n",
    "           df[\"coolCount\"], df[\"funnyCount\"], df[\"maxReviews\"]]\n",
    "headers = [\"max_tfidf\", \"reviewLength\", \"numCount\", \n",
    "           \"symCount\", \"adjProp\", \"avgSentiment\", \"nounProp\", \n",
    "           \"hasProfile\", \"postCount\", \"usefulCount\", \n",
    "           \"coolCount\", \"funnyCount\", \"maxReviews\"]\n",
    "fs3 = pd.concat(columns, axis=1, keys=headers)\n",
    "\n",
    "# feature set 4 - both categories, top 10 overall based on correlation and t-test\n",
    "columns = [df[\"hasProfile\"], df[\"postCount\"], \n",
    "           df[\"usefulCount\"], df[\"max_tfidf\"], df[\"reviewLength\"], \n",
    "           df[\"coolCount\"], df[\"funnyCount\"], df[\"maxReviews\"], \n",
    "           df[\"numCount\"], df[\"symCount\"]]\n",
    "headers = [\"hasProfile\", \"postCount\", \"usefulCount\", \"max_tfidf\", \n",
    "           \"reviewLength\", \"coolCount\", \"funnyCount\", \n",
    "           \"maxReviews\", \"numCount\", \"symCount\"]\n",
    "fs4 = pd.concat(columns, axis=1, keys=headers)\n",
    "\n",
    "# feature set 5 - both categories, top 5 overall based on correlation and t-test\n",
    "columns = [df[\"hasProfile\"], df[\"postCount\"], \n",
    "           df[\"usefulCount\"], df[\"max_tfidf\"], df[\"reviewLength\"]]\n",
    "headers = [\"hasProfile\", \"postCount\", \"usefulCount\", \n",
    "           \"max_tfidf\", \"reviewLength\"]\n",
    "fs5 = pd.concat(columns, axis=1, keys=headers)\n",
    "\n",
    "# feature set 6 - both categories, top 5 overall based on feature importance of model trained on FS3 (all features)\n",
    "columns = [df[\"usefulCount\"], df[\"postCount\"], \n",
    "           df[\"avgSentiment\"], df[\"reviewLength\"], df[\"max_tfidf\"]]\n",
    "headers = [\"usefulCount\", \"postCount\", \n",
    "           \"avgSentiment\", \"reviewLength\", \"max_tfidf\"]\n",
    "fs6 = pd.concat(columns, axis=1, keys=headers)\n",
    "\n",
    "# feature set 7 - both categories, top 10 overall based on feature importance of model trained on FS3 (all features)\n",
    "columns = [df[\"usefulCount\"], df[\"postCount\"], df[\"avgSentiment\"], \n",
    "           df[\"reviewLength\"], df[\"max_tfidf\"], df[\"nounProp\"], \n",
    "           df[\"adjProp\"], df[\"hasProfile\"], df[\"numCount\"], df[\"coolCount\"]]\n",
    "headers = [\"usefulCount\", \"postCount\", \"avgSentiment\", \"reviewLength\", \n",
    "           \"max_tfidf\", \"nounProp\", \"adjProp\", \"hasProfile\", \"numCount\", \"coolCount\"]\n",
    "fs7 = pd.concat(columns, axis=1, keys=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10-fold Stratified Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate average model performance using stratified 10-fold cross-validation\n",
    "\n",
    "###\n",
    "# SPECIFY FEATURE SET HERE\n",
    "features = fs4\n",
    "###\n",
    "\n",
    "max_depth = 6\n",
    "number_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits = number_of_folds, shuffle = True)\n",
    "skf.get_n_splits(features, labels)\n",
    "print(skf)\n",
    "count = 1\n",
    "\n",
    "acc_sum = 0\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f_sum = 0\n",
    "balanced_sum = 0\n",
    "\n",
    "# loop through train/test data splits\n",
    "for train_index, test_index in skf.split(features, labels):\n",
    "    x_train, x_test = features.iloc[train_index], features.iloc[test_index]\n",
    "    y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "\n",
    "    ###\n",
    "    # 1. re-sample training set ONLY (keep test data class distribution the same)\n",
    "    ###\n",
    "    #Â Comment out the following line to train model on natural class distribution of fake/genuine reviews\n",
    "    x_train, y_train = resample(x_train, y_train)\n",
    "    \n",
    "    # check effect of resampling on the number of fake and genuine review samples in training dataset\n",
    "    #print(\"Fake reviews in training set:\")\n",
    "    #print(len(y_train[y_train == 'Y']))\n",
    "    #print(\"Genuine reviews in training set:\")\n",
    "    #print(len(y_train[y_train == 'N']))\n",
    "    #print()\n",
    "\n",
    "    ###\n",
    "    # 2. train classifier\n",
    "    ###\n",
    "    # use the following line for training a fully grown tree with no hyper-parameter tuning\n",
    "    # dt = DecisionTreeClassifier()\n",
    "    # use the following line for training a tree with hyper-parameter tuning applied\n",
    "    dt = DecisionTreeClassifier(max_depth = max_depth)\n",
    "    \n",
    "    # build decision tree from training data\n",
    "    dt.fit(x_train, y_train)\n",
    "\n",
    "    ###\n",
    "    # 3. predict test data\n",
    "    predictions = dt.predict(x_test)\n",
    "    ###\n",
    "    \n",
    "    # create confusion matrix based on predictions vs. actual labels of reviews\n",
    "    confusion_matrix = metrics.confusion_matrix(y_test, predictions, labels=['N','Y'])\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, predictions).ravel()\n",
    "    \n",
    "    ###\n",
    "    # 4. calculate evaluation metrics\n",
    "    ###\n",
    "    \n",
    "    # a. accuracy\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    # b. precision\n",
    "    if(tp == 0 and fp == 0):\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    # c. recall\n",
    "    if(tp == 0 and fn == 0):\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    # d. f score\n",
    "    if(precision == 0 and recall == 0):\n",
    "        f_score = 0\n",
    "    else:\n",
    "        f_score = (2 * precision * recall) / (precision + recall)\n",
    "    if(tn == 0 and fp == 0):\n",
    "        true_negative_rate = 0\n",
    "    else:\n",
    "        true_negative_rate = tn / (tn + fp)\n",
    "    # e. balanced accuracy\n",
    "    balanced_accuracy = (true_negative_rate + recall) / 2\n",
    "    \n",
    "    # get feature importances - Gini importance\n",
    "    # it represents the normalised total reduction of the criterion by a given feature\n",
    "    #importances = dict(zip(features.columns, dt.feature_importances_))\n",
    "    #feature_importances = dt.feature_importances_\n",
    "    #for index in range(len(feature_importances)):\n",
    "        #list_of_feature_importance[index] += feature_importances[index]\n",
    "        #print(feature_importances[index])\n",
    "    #print()\n",
    "\n",
    "    print(\"Run \", count, \": \")\n",
    "    print(\"Overall accuracy: \", accuracy * 100)\n",
    "    print(\"Balanced accuracy: \", balanced_accuracy * 100)\n",
    "    print(\"Precision: \", precision * 100)\n",
    "    print(\"Recall: \", recall * 100)\n",
    "    print(\"TNR: \", true_negative_rate * 100)\n",
    "    print(\"F Score: \", f_score * 100)\n",
    "    print()\n",
    "    count += 1\n",
    "    \n",
    "    acc_sum += accuracy\n",
    "    precision_sum += precision\n",
    "    recall_sum += recall\n",
    "    f_sum += f_score\n",
    "    balanced_sum += balanced_accuracy\n",
    "\n",
    "print()\n",
    "print(\"Average accuracy over\", number_of_folds, \"runs: \", round(acc_sum / number_of_folds * 100, 2))\n",
    "print(\"Average balanced accuracy over\", number_of_folds, \"runs: \", round(balanced_sum / number_of_folds * 100, 2))\n",
    "print(\"Average precision over\", number_of_folds, \"runs: \", round(precision_sum / number_of_folds * 100, 2))\n",
    "print(\"Average recall over\", number_of_folds, \"runs: \", round(recall_sum / number_of_folds * 100, 2))\n",
    "print(\"Average F score over\", number_of_folds, \"runs: \", round(f_sum / number_of_folds * 100, 2))\n",
    "print()\n",
    "\n",
    "# get average feature importance of each feature during k-fold CV\n",
    "#for index in range(len(list_of_feature_importance)):\n",
    "    #list_of_feature_importance[index] = list_of_feature_importance[index] / number_of_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter optimisation (max_depth feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "# SPECIFY FEATURE SET HERE\n",
    "features = fs4\n",
    "###\n",
    "\n",
    "number_of_folds = 10\n",
    "skf = StratifiedKFold(n_splits = number_of_folds, shuffle = True)\n",
    "skf.get_n_splits(features, labels)\n",
    "\n",
    "# specify range of values here to test for hyper-parameter optimisation\n",
    "max_depth_range = (range(3,26))\n",
    "max_depth_avg_f1 = []\n",
    "\n",
    "for max_depth in max_depth_range:\n",
    "    \n",
    "    print(\"Performing 10-fold cross-validation for max_depth: \", max_depth)\n",
    "    \n",
    "    f_sum = 0\n",
    "    \n",
    "    # loop through train/test data splits\n",
    "    for train_index, test_index in skf.split(features, labels):\n",
    "        x_train, x_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "        \n",
    "        # balance training set\n",
    "        x_train, y_train = resample(x_train, y_train)\n",
    "        \n",
    "        dt = DecisionTreeClassifier(max_depth = max_depth)\n",
    "        dt.fit(x_train, y_train)\n",
    "        \n",
    "        predictions = dt.predict(x_test)\n",
    "        confusion_matrix = metrics.confusion_matrix(y_test, predictions, labels=['N','Y'])\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(y_test, predictions).ravel()\n",
    "        \n",
    "        # b. precision\n",
    "        if(tp == 0 and fp == 0):\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = tp / (tp + fp)\n",
    "        # c. recall\n",
    "        if(tp == 0 and fn == 0):\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = tp / (tp + fn)\n",
    "        # d. f score\n",
    "        if(precision == 0 and recall == 0):\n",
    "            f_score = 0\n",
    "        else:\n",
    "            f_score = (2 * precision * recall) / (precision + recall)\n",
    "        \n",
    "        f_sum += f_score\n",
    "    avg_score = round(f_sum / number_of_folds * 100, 2)\n",
    "    #print(\"Average F score over\", number_of_folds, \"runs for max_depth\", max_depth, \"was:\", avg_score)\n",
    "    #print()\n",
    "    #print(\"...\")\n",
    "    #print()\n",
    "    max_depth_avg_f1.append(avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dict(zip(max_depth_range, max_depth_avg_f1))\n",
    "print(scores)\n",
    "\n",
    "# plot the average F1 score against the max_depth value on a scatter plot \n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(scores.keys(), scores.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get feature importances and decision tree characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print feature importances in construction of tree\n",
    "\n",
    "# Also known as Gini importance, it represents the normalised total reduction of the criterion by a given feature\n",
    "# importances add up to 1.0\n",
    "importances = dict(zip(features.columns, dt.feature_importances_))\n",
    "for value in importances:\n",
    "    print(value, \":\", importances[value])\n",
    "print()\n",
    "    \n",
    "# Print decision tree characteristics\n",
    "\n",
    "print(\"decision tree characteristics: \")\n",
    "print(\"depth of decision tree: \", dt.get_depth())\n",
    "print(\"number of leaves: \", dt.get_n_leaves())\n",
    "print()\n",
    "dt.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialise classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialise decision tree classifier and train/test data and test reviews for front end\n",
    "\n",
    "# serialise classifier\n",
    "with open('dt_final.pkl', 'wb') as f:\n",
    "    pickle.dump(dt, f)\n",
    "\n",
    "# serialise train/test data\n",
    "with open('x_train.pkl', 'wb') as f:\n",
    "    pickle.dump(x_train, f)\n",
    "with open('x_test.pkl', 'wb') as f:\n",
    "    pickle.dump(x_test, f)\n",
    "with open('y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)    \n",
    "with open('y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "    \n",
    "# serialise reviews associated with test data\n",
    "df1 = df[df.index.isin(x_test.index)]\n",
    "with open('df_test.pkl', 'wb') as f:\n",
    "    pickle.dump(df1, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
