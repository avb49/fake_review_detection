{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering (Extraction and Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import nltk\n",
    "# a tokenizer will be used for pre-processing the review strings for feature extraction\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "# Vader and its lexicon will be used for the extraction of a sentiment analysis-based feature\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "nltk.download('vader_lexicon')\n",
    "# spaCy will be used for tokenising (into words) reviews and tagging parts of speech within them\n",
    "import spacy\n",
    "# from scikit-learn import the TF-IDF vectoriser to be used for the n-grams feature\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# stats package will be used for feature selection\n",
    "from scipy import stats\n",
    "# pyplot will be used for creating boxplots\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe to be used for feature engineering\n",
    "\n",
    "# load dataframe with no features (output by data preparation code)\n",
    "### SET OWN PATH HERE\n",
    "### \n",
    "path = \"/Users/artembutbaev/OneDrive/University of Bath 20-21 \" + \\\n",
    "\"(Year 4)/CM - Individual Project/2. Code/Yelp data/reviews.pkl\"\n",
    "###\n",
    "###\n",
    "\n",
    "df = pd.read_pickle(path)\n",
    "\n",
    "###\n",
    "# Extract Feature 1 - Review length (number of characters in review string)\n",
    "###\n",
    "\n",
    "number_of_reviews = len(df)\n",
    "reviewLengthList = []\n",
    "for review_index in range(number_of_reviews):\n",
    "    length_of_review = len(df[\"reviewContent\"].iloc[review_index])\n",
    "    reviewLengthList.append(length_of_review)\n",
    "df[\"reviewLength\"] = reviewLengthList\n",
    "df.reset_index(drop = True)\n",
    "\n",
    "###\n",
    "# Extract Feature 2 - Maximum number of reviews written by reviewer in a single day\n",
    "###\n",
    "\n",
    "# create series with all unique reviewerID values\n",
    "unique_reviewers = pd.Series(df[\"reviewerID\"].unique())\n",
    "# create empty column to be used for filling in values\n",
    "df[\"maxReviews\"] = np.nan\n",
    "\n",
    "for reviewer_index in range(0, len(unique_reviewers)):\n",
    "    \n",
    "    # 1. get maximum number of reviews posted in a single day for given reviewer\n",
    "    max_reviews_reviewer = df[\"date\"][df[\"reviewerID\"] == unique_reviewers[reviewer_index]].value_counts()[0]\n",
    "    # 2. update the column \"maxReviews\" for every row with the given reviewer\n",
    "    df.loc[df.reviewerID == unique_reviewers[reviewer_index], \"maxReviews\"] = max_reviews_reviewer\n",
    "    \n",
    "# convert column to integers after all values filled in\n",
    "df[\"maxReviews\"] = df[\"maxReviews\"].apply(np.int64)\n",
    "\n",
    "###\n",
    "# Extract Feature 3 - Average sentiment of review\n",
    "###\n",
    "\n",
    "# initialise Sentiment Intensity Analyzer (SIA)\n",
    "analyzer = SIA()\n",
    "#Â create empty list to store all sentiment values\n",
    "sentiment_list = []\n",
    "\n",
    "# calculate for each review an average sentiment \n",
    "# value based on individual sentences\n",
    "for review in df['reviewContent']:\n",
    "\n",
    "    # 1. split review into individual sentences (tokenise)\n",
    "    review_sentences = tokenize.sent_tokenize(review)\n",
    "    \n",
    "    # 2. get the average sentiment for the review \n",
    "    # (average sentiment of sentences in review)\n",
    "    total_sentiment = 0.0\n",
    "    for sentence in review_sentences:\n",
    "        sentiment_score = analyzer.polarity_scores(sentence)\n",
    "        total_sentiment += sentiment_score[\"compound\"]\n",
    "    \n",
    "    # calculate average sentiment\n",
    "    average_sentiment = round(total_sentiment / no_sentences, 4)\n",
    "    \n",
    "    # 3. append average sentiment to sentiment_list\n",
    "    sentiment_list.append(average_sentiment)\n",
    "    \n",
    "# add new feature to dataframe\n",
    "df[\"avgSentiment\"] = sentiment_list\n",
    "\n",
    "###\n",
    "# Extract Features 4-9 - Parts of Speech (PoS)\n",
    "###\n",
    "\n",
    "# load parts of speech tagger from spaCy library\n",
    "tagger = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "noun_proportion_list = []\n",
    "adj_proportion_list = []\n",
    "verb_proportion_list = []\n",
    "propnoun_proportion_list = []\n",
    "num_count_list = []\n",
    "symbol_count_list = []\n",
    "\n",
    "for review in df['reviewContent']:\n",
    "        \n",
    "    # 1. tokenise and tag review\n",
    "    tagged_review = tagger(review)\n",
    "    \n",
    "    # get count of tokens (words) in review\n",
    "    token_count = len(tagged_review)\n",
    "    \n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    propnoun_count = 0\n",
    "    num_count = 0\n",
    "    symbol_count = 0\n",
    "    \n",
    "    # 2. Get counts of nouns, adjectives, verbs, \n",
    "    # proper nouns, numbers and symbols\n",
    "    for token in tagged_review:\n",
    "        \n",
    "        if(token.pos_ == \"NOUN\"):\n",
    "            noun_count += 1\n",
    "        if(token.pos_ == \"ADJ\"):\n",
    "            adj_count += 1\n",
    "        if(token.pos_ == \"VERB\"):\n",
    "            verb_count += 1\n",
    "        if(token.pos_ == \"PROPN\"):\n",
    "            propnoun_count += 1\n",
    "        if(token.pos_ == \"NUM\"):\n",
    "            num_count += 1\n",
    "        if(token.pos_ == \"SYM\"):\n",
    "            symbol_count += 1\n",
    "        \n",
    "    # 3. append proportions to lists\n",
    "    noun_proportion_list.append(noun_count / token_count)\n",
    "    adj_proportion_list.append(adj_count / token_count)\n",
    "    verb_proportion_list.append(verb_count / token_count)\n",
    "    propnoun_proportion_list.append(propnoun_count / token_count)\n",
    "    num_count_list.append(num_count)\n",
    "    symbol_count_list.append(symbol_count)\n",
    "    \n",
    "# add new features to dataframe\n",
    "df[\"nounProp\"] = noun_proportion_list\n",
    "df[\"adjProp\"] = adj_proportion_list\n",
    "df[\"verbProp\"] = verb_proportion_list\n",
    "df[\"propernounProp\"] = propnoun_proportion_list\n",
    "df[\"numCount\"] = num_count_list\n",
    "df[\"symCount\"] = symbol_count_list\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "###\n",
    "# Extract Feature 10 - Maximum TF-IDF score for a given review\n",
    "###\n",
    "\n",
    "# create corpus - a list of all reviews\n",
    "corpus = df[\"reviewContent\"].tolist()\n",
    "# initialise vectoriser and assign stop word list\n",
    "# if a word appears in more than 85% of documents, it will\n",
    "# not be part of the vector\n",
    "vectoriser = TfidfVectorizer(max_df = 0.85, stop_words = \"english\")\n",
    "# fit and transform TF-IDF model on corpus\n",
    "vocabulary = vectoriser.fit_transform(corpus)\n",
    "# get dimensions of vocabulary: X is number of reviews, \n",
    "# where Y is the TF-IDF value\n",
    "print(vocabulary.shape)\n",
    "\n",
    "max_tfidf_list = []\n",
    "# loop through all reviews and \n",
    "# calculate their maximum TF-IDF value\n",
    "for document_index in range(len(corpus)):\n",
    "        \n",
    "    max_value = vocabulary[document_index].max()\n",
    "    max_tfidf_list.append(max_value)\n",
    "    \n",
    "# add new feature to dataframe\n",
    "df[\"max_tfidf\"] = max_tfidf_list\n",
    "\n",
    "###\n",
    "# Feature X - Reviewer membership length (skipped)\n",
    "###\n",
    "\n",
    "# length of time reviewer has been a member of the online platform (Yelp)\n",
    "# this feature has been skipped because the reviewer table provided in the data is incomplete, \n",
    "# with over 40,000 of the 60,019 reviewerIDs unmatched in the reviews table\n",
    "\n",
    "###\n",
    "# Feature X - Friend count (skipped)\n",
    "###\n",
    "# skipped due to same reason as the previous feature (unmatched reviewerIDs)\n",
    "\n",
    "###\n",
    "# Extract Feature 11 - Count of reviews posted by reviewer\n",
    "###\n",
    "\n",
    "# create series with all unique reviewerID values\n",
    "unique_reviewers = pd.Series(df[\"reviewerID\"].unique())\n",
    "print(\"Number of unique reviewers: \", len(unique_reviewers))\n",
    "# create empty column to be used for filling in values\n",
    "df[\"postCount\"] = np.nan\n",
    "\n",
    "for reviewer_index in range(0, len(unique_reviewers)):\n",
    "        \n",
    "    # 1. get number of reviews posted by a given reviewer\n",
    "    posts_by_reviewer = df[\"reviewerID\"][df[\"reviewerID\"] == unique_reviewers[reviewer_index]].value_counts().sum()\n",
    "    \n",
    "    # 2. update the column \"postCount\" for every row with the given reviewer\n",
    "    df.loc[df.reviewerID == unique_reviewers[reviewer_index], \"postCount\"] = posts_by_reviewer\n",
    "    \n",
    "# convert column to integers after all values filled in\n",
    "df[\"postCount\"] = df[\"postCount\"].apply(np.int64)\n",
    "\n",
    "# Pickle (serialise) dataframe with all features ready to be used in model evaluation\n",
    "df.to_pickle(\"./df3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe for feature selection\n",
    "\n",
    "# load dataframe with all features extracted\n",
    "### SET OWN PATH HERE\n",
    "###\n",
    "path = \"/Users/artembutbaev/OneDrive/University of Bath 20-21 \" + \\\n",
    "\"(Year 4)/CM - Individual Project/2. Code/Model Building/df4.pkl\"\n",
    "###\n",
    "###\n",
    "\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "# Define functions to be used for feature selection\n",
    "\n",
    "# print quartiles and spread of data points for a given feature\n",
    "def print_quartiles(df, feature, bins_count):\n",
    "    \n",
    "    df_genuine = df[df[\"flagged\"] == 'N']\n",
    "    df_fake = df[df[\"flagged\"] == 'Y']\n",
    "    \n",
    "    feature_genuine = df_genuine[feature]\n",
    "    feature_fake = df_fake[feature]\n",
    "\n",
    "    print(feature + \" - Genuine reviews: \")\n",
    "    print(\"Median (Q2): \", np.quantile(feature_genuine, .50)) \n",
    "    print(\"Q1: \", np.quantile(feature_genuine, .25))\n",
    "    print(\"Q3: \", np.quantile(feature_genuine, .75))\n",
    "    print(\"IQR: \", np.quantile(feature_genuine, .75) - np.quantile(feature_genuine, .25))\n",
    "    print()\n",
    "    print(feature + \" - Fake reviews: \")\n",
    "    print(\"Median (Q2): \", np.quantile(feature_fake, .50)) \n",
    "    print(\"Q1: \", np.quantile(feature_fake, .25)) \n",
    "    print(\"Q3: \", np.quantile(feature_fake, .75)) \n",
    "    print(\"IQR: \", np.quantile(feature_fake, .75) - np.quantile(feature_fake, .25)) \n",
    "\n",
    "    # value counts (in bins) - use for bar charts to visually compare\n",
    "    print(\"Total: \", len(feature_genuine))\n",
    "    print(\"Value counts - Genuine reviews: \")\n",
    "    print(feature_genuine.value_counts(bins=bins_count, dropna=False))\n",
    "    print()\n",
    "    print(\"Total: \", len(feature_fake))\n",
    "    print(\"Value counts - Fake reviews: \")\n",
    "    print(feature_fake.value_counts(bins=bins_count, dropna=False))\n",
    "\n",
    "# create a boxplot for a given feature and an attribute to group it by (typically, the label)\n",
    "def create_boxplot(df, feature, group_by):\n",
    "    \n",
    "    boxplot = df.boxplot(column=[feature], by=group_by, return_type=None, showfliers=False)\n",
    "    figure = boxplot.get_figure()\n",
    "    figure.suptitle('')\n",
    "    plt.show()\n",
    "    \n",
    "# Print quartiles and create associated boxplots\n",
    "\n",
    "# 1. reviewLength feature\n",
    "print_quartiles(df, \"reviewLength\", 5)\n",
    "create_boxplot(df, \"reviewLength\", \"flagged\")\n",
    "# 2. maxReviews feature\n",
    "print_quartiles(df, \"maxReviews\", 1)\n",
    "create_boxplot(df, \"maxReviews\", \"flagged\")\n",
    "# 3. avgSentiment feature\n",
    "print_quartiles(df, \"avgSentiment\", 5)\n",
    "create_boxplot(df, \"avgSentiment\", \"flagged\")\n",
    "# 4. adjective proportion feature\n",
    "print_quartiles(df, \"adjProp\", 10)\n",
    "create_boxplot(df, \"adjProp\", \"flagged\")\n",
    "# 5. max tf-idf feature\n",
    "print_quartiles(df, \"max_tfidf\", 5)\n",
    "create_boxplot(df, \"max_tfidf\", \"flagged\")\n",
    "# 6. post count feature\n",
    "print_quartiles(df, \"postCount\", 5)\n",
    "create_boxplot(df, \"postCount\", \"flagged\")\n",
    "# 7. noun proportion feature\n",
    "print_quartiles(df, \"nounProp\", 5)\n",
    "create_boxplot(df, \"nounProp\", \"flagged\")\n",
    "# 8. verb proportion feature\n",
    "print_quartiles(df, \"verbProp\", 5)\n",
    "create_boxplot(df, \"verbProp\", \"flagged\")\n",
    "# 9. propernoun proportion feature\n",
    "print_quartiles(df, \"propernounProp\", 5)\n",
    "create_boxplot(df, \"propernounProp\", \"flagged\")\n",
    "# 10. numcount feature\n",
    "print_quartiles(df, \"numCount\", 10)\n",
    "create_boxplot(df, \"numCount\", \"flagged\")\n",
    "# 11. symcount feature\n",
    "print_quartiles(df, \"symCount\", 10)\n",
    "create_boxplot(df, \"symCount\", \"flagged\")\n",
    "# 12. hasProfile feature\n",
    "print_quartiles(df, \"hasProfile\", 1)\n",
    "create_boxplot(df, \"hasProfile\", \"flagged\")\n",
    "# 13. usefulCount feature\n",
    "print_quartiles(df, \"usefulCount\", 1)\n",
    "create_boxplot(df, \"usefulCount\", \"flagged\")\n",
    "# 14. coolCount feature\n",
    "print_quartiles(df, \"coolCount\", 1)\n",
    "create_boxplot(df, \"coolCount\", \"flagged\")\n",
    "# 15. funnyCount feature\n",
    "print_quartiles(df, \"funnyCount\", 1)\n",
    "create_boxplot(df, \"funnyCount\", \"flagged\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates point-biserial correlation between labels and a given feature\n",
    "def point_biserial(labels, feature):\n",
    "    \n",
    "    # convert feature to numpy array\n",
    "    feature = np.asarray(feature)\n",
    "    \n",
    "    # Output: -1 indicates a perfect negative association, \n",
    "    # +1 indicates a perfect positive association, and 0 indicates no association\n",
    "    #return stats.pointbiserialr(labels, feature)\n",
    "    result, p_value = stats.pointbiserialr(labels, feature)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate point-biserial correlation for each feature and the associated labels\n",
    "###\n",
    "\n",
    "# convert labels to binary integers (\"Y\" = 1, \"N\" = 0)\n",
    "labels = df[\"flagged\"].tolist()\n",
    "labels_binary = list(labels)\n",
    "for label_index in range(len(labels)):\n",
    "    if(labels[label_index] == 'Y'):\n",
    "        labels_binary[label_index] = 1\n",
    "    elif(labels[label_index] == 'N'):\n",
    "        labels_binary[label_index] = 0\n",
    "# convert labels list to numpy array\n",
    "labels_binary = np.asarray(labels_binary)\n",
    "\n",
    "# feature 1 - length of a review\n",
    "feature_1 = df[\"reviewLength\"].tolist()\n",
    "print(\"reviewLength feature: \")\n",
    "print(point_biserial(labels_binary, feature_1))\n",
    "# feature 2 - maximum number of reviews by reviewer in a single day\n",
    "feature_2 = df[\"maxReviews\"].tolist()\n",
    "print(\"maxReviews feature: \")\n",
    "print(point_biserial(labels_binary, feature_2))\n",
    "# feature 3 - average sentiment of a review\n",
    "feature_3 = df[\"avgSentiment\"].tolist()\n",
    "print(\"avgSentiment feature: \")\n",
    "print(point_biserial(labels_binary, feature_3))\n",
    "# feature 4 - proportion of nouns\n",
    "feature_4 = df[\"nounProp\"].tolist()\n",
    "print(\"nounProp feature: \")\n",
    "print(point_biserial(labels_binary, feature_4))\n",
    "# feature 5 - proportion of adjectives\n",
    "feature_5 = df[\"adjProp\"].tolist()\n",
    "print(\"adjProp feature: \")\n",
    "print(point_biserial(labels_binary, feature_5))\n",
    "# feature 6 - proportion of verbs\n",
    "feature_6 = df[\"verbProp\"].tolist()\n",
    "print(\"verbProp feature: \")\n",
    "print(point_biserial(labels_binary, feature_6))\n",
    "# feature 7 - proportion of proper nouns\n",
    "feature_7 = df[\"propernounProp\"].tolist()\n",
    "print(\"propernounProp feature: \")\n",
    "print(point_biserial(labels_binary, feature_7))\n",
    "# feature 8 - max tf-idf\n",
    "feature_8 = df[\"max_tfidf\"].tolist()\n",
    "print(\"max_tfidf feature: \")\n",
    "print(point_biserial(labels_binary, feature_8))\n",
    "# feature 9 - count of numbers in review\n",
    "feature_9 = df[\"numCount\"].tolist()\n",
    "print(\"numCount feature: \")\n",
    "print(point_biserial(labels_binary, feature_9))\n",
    "# feature 10 - count of symbols in review\n",
    "feature_10 = df[\"symCount\"].tolist()\n",
    "print(\"symCount feature: \")\n",
    "print(point_biserial(labels_binary, feature_10))\n",
    "# feature 11 - count of posts\n",
    "feature_11 = df[\"postCount\"].tolist()\n",
    "print(\"postCount feature: \")\n",
    "print(point_biserial(labels_binary, feature_11))\n",
    "# feature 12 - hasProfile\n",
    "feature_12 = df[\"hasProfile\"].tolist()\n",
    "print(\"hasProfile feature: \")\n",
    "print(point_biserial(labels_binary, feature_12))\n",
    "# feature 13 - usefulCount\n",
    "feature_13 = df[\"usefulCount\"].tolist()\n",
    "print(\"usefulCount feature: \")\n",
    "print(point_biserial(labels_binary, feature_13))\n",
    "# feature 14 - coolCount\n",
    "feature_14 = df[\"coolCount\"].tolist()\n",
    "print(\"coolCount feature: \")\n",
    "print(point_biserial(labels_binary, feature_14))\n",
    "# feature 15 - funnyCount\n",
    "feature_15 = df[\"funnyCount\"].tolist()\n",
    "print(\"funnyCount feature: \") \n",
    "print(point_biserial(labels_binary, feature_15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate t-test for groups of features for genuine and fake review\n",
    "###\n",
    "\n",
    "df_genuine = df[df[\"flagged\"] == \"N\"]\n",
    "df_fake = df[df[\"flagged\"] == \"Y\"]\n",
    "\n",
    "# 1. review length\n",
    "feature_genuine = df_genuine[\"reviewLength\"]\n",
    "feature_fake = df_fake[\"reviewLength\"]\n",
    "print('\\033[1m' + \"1. Review length:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 2. maximum reviews posted in a single day by reviewer\n",
    "feature_genuine = df_genuine[\"maxReviews\"]\n",
    "feature_fake = df_fake[\"maxReviews\"]\n",
    "print('\\033[1m' + \"2. Maximum reviews posted by reviewer in a single day:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 3. average sentiment of sentences in review\n",
    "feature_genuine = df_genuine[\"avgSentiment\"]\n",
    "feature_fake = df_fake[\"avgSentiment\"]\n",
    "print('\\033[1m' + \"3. Average sentiment of sentences in review:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 4. noun proportion out of all words in review\n",
    "feature_genuine = df_genuine[\"nounProp\"]\n",
    "feature_fake = df_fake[\"nounProp\"]\n",
    "print('\\033[1m' + \"4. Noun proportion:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 5. adjective proportion out of all words in review\n",
    "feature_genuine = df_genuine[\"adjProp\"]\n",
    "feature_fake = df_fake[\"adjProp\"]\n",
    "print('\\033[1m' + \"5. Adjective proportion:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 6. verb proportion out of all words in review\n",
    "feature_genuine = df_genuine[\"verbProp\"]\n",
    "feature_fake = df_fake[\"verbProp\"]\n",
    "print('\\033[1m' + \"6. Verb proportion:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 7. proper noun proportion out of all words in review\n",
    "feature_genuine = df_genuine[\"propernounProp\"]\n",
    "feature_fake = df_fake[\"propernounProp\"]\n",
    "print('\\033[1m' + \"7. Proper noun proportion:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 8. maximum tf-idf value in review\n",
    "feature_genuine = df_genuine[\"max_tfidf\"]\n",
    "feature_fake = df_fake[\"max_tfidf\"]\n",
    "print('\\033[1m' + \"8. Maximum tf-idf value:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 9. maximum tf-idf value in review\n",
    "feature_genuine = df_genuine[\"postCount\"]\n",
    "feature_fake = df_fake[\"postCount\"]\n",
    "print('\\033[1m' + \"9. Post count:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 10. symbol count in review\n",
    "feature_genuine = df_genuine[\"symCount\"]\n",
    "feature_fake = df_fake[\"symCount\"]\n",
    "print('\\033[1m' + \"10. Symbol count:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 11. number count in review\n",
    "feature_genuine = df_genuine[\"numCount\"]\n",
    "feature_fake = df_fake[\"numCount\"]\n",
    "print('\\033[1m' + \"11. Number count:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 12. HasProfile\n",
    "feature_genuine = df_genuine[\"hasProfile\"]\n",
    "feature_fake = df_fake[\"hasProfile\"]\n",
    "print('\\033[1m' + \"12. Has reviewer got a profile:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 13. usefulCount\n",
    "feature_genuine = df_genuine[\"usefulCount\"]\n",
    "feature_fake = df_fake[\"usefulCount\"]\n",
    "print('\\033[1m' + \"13. Useful count:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 14. coolCount\n",
    "feature_genuine = df_genuine[\"coolCount\"]\n",
    "feature_fake = df_fake[\"coolCount\"]\n",
    "print('\\033[1m' + \"14. Cool count:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)\n",
    "# 15. funnyCount\n",
    "feature_genuine = df_genuine[\"funnyCount\"]\n",
    "feature_fake = df_fake[\"funnyCount\"]\n",
    "print('\\033[1m' + \"15. Funny count:\" + '\\033[0m')\n",
    "statistic, p_value = stats.ttest_ind(feature_genuine, feature_fake)\n",
    "print(statistic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
